# -*- coding: utf-8 -*-
"""Copy of Yet another copy of Yet another copy of Untitled5.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EwVDM0UYzej1C7o41lODR_hCVGpdor-U
"""

pip install torch

pip install torchvision

import torch
from torchvision import datasets
from torchvision import transforms
import matplotlib.pyplot as plt
from torch.utils.data import Dataset, DataLoader
from torchvision.io import read_image

pip install patool

import patoolib

patoolib.extract_archive("hymenoptera_data.zip", outdir=".")



ls

"""# New Section"""

from google.colab import files

uploaded = files.upload()

for fn in uploaded.keys():
  print('User uploaded file "{name}" with length {length} bytes'.format(
      name=fn, length=len(uploaded[fn])))

imgTrs = ColorTransWithFlip()
data_transform = transforms.Compose([
    transforms.Lambda(lambda img:imgTrs.lab2rgb(img)),
    # Resize the images to 224*224
    transforms.Resize(size=(224, 224)),
           # Turn the image into a torch.Tensor
    transforms.ToTensor() # this also converts all pixel values from 0 to 255 to be between 0.0 and 1.0
])

import glob
from PIL import Image
class AntBeeImgTrainDataSet(Dataset):
  def __init__(self,transfroms):
        self.imgs_path = "./hymenoptera_data/train/"
        self.file_list = glob.glob(self.imgs_path + "*")
        print(self.file_list)
        self.data = []
        self.transform = transfroms
        for class_path in self.file_list:
            class_name = class_path.split("/")[-1]
            for img_path in glob.glob(class_path + "/*.jpg"):
                self.data.append([img_path, class_name])
        print(self.data)
        self.class_map = {"ants" : 0, "bees": 1}
  def __len__(self):
        return len(self.data)
  def __getitem__(self, idx):
        img_path, class_name = self.data[idx]
        img = Image.open(img_path)
        imgTensor = self.transform(img)
        class_id = self.class_map[class_name]
        class_id = torch.tensor([class_id])
        return imgTensor, class_id


class AntBeeImgValDataSet(Dataset):
  def __init__(self,transfroms):
        self.imgs_path = "./hymenoptera_data/val/"
        file_list = glob.glob(self.imgs_path + "*")
        print(file_list)
        self.data = []
        self.transform = transfroms
        for class_path in file_list:
            class_name = class_path.split("/")[-1]
            for img_path in glob.glob(class_path + "/*.jpg"):
                self.data.append([img_path, class_name])
        print(self.data)
        self.class_map = {"ants" : 0, "bees": 1}
  def __len__(self):
        return len(self.data)
  def __getitem__(self, idx):
        img_path, class_name = self.data[idx]
        img = Image.open(img_path)
        imgTensor = self.transform(img)
        class_id = self.class_map[class_name]
        class_id = torch.tensor([class_id])
        return imgTensor, class_id

imgs_path = "./hymenoptera_data/train/"
 file_list = glob.glob(imgs_path + "*")
 print(file_list)



trainDataSet = AntBeeImgTrainDataSet(data_transform)
print(trainDataSet)
testDataSet = AntBeeImgValDataSet(data_transform)
print(testDataSet)

loader = torch.utils.data.DataLoader(dataset = trainDataSet,
                                     batch_size = 32,
                                     shuffle = True)
testLoader = torch.utils.data.DataLoader(dataset = testDataSet,
                                     batch_size = 32,
                                     shuffle = True)

import torch.nn.functional as F

class MT19AIE275AutoEncoder(torch.nn.Module):
    def __init__(self):
        super(MT19AIE275AutoEncoder, self).__init__()
         ## encoder layers ##
        # conv layer (depth from 3 --> 16), 3x3 kernels
        self.conv1 = torch.nn.Conv2d(3, 16, 3, padding=1)
        # conv layer (depth from 16 --> 4), 3x3 kernels
        self.conv2 = torch.nn.Conv2d(16, 4, 3, padding=1)
        # pooling layer to reduce x-y dims by two; kernel and stride of 2
        self.pool = torch.nn.MaxPool2d(2, 2)

        ## decoder layers ##
        ## a kernel of 2 and a stride of 2 will increase the spatial dims by 2
        self.t_conv1 = torch.nn.ConvTranspose2d(4, 16, 2, stride=2)
        self.t_conv2 = torch.nn.ConvTranspose2d(16, 3, 2, stride=2)

    def forward(self, x):
        ## encode ##
        # add hidden layers with relu activation function
        # and maxpooling after
        x = F.relu(self.conv1(x))
        x = self.pool(x)
        # add second hidden layer
        x = F.relu(self.conv2(x))
        x = self.pool(x)  # compressed representation

        ## decode ##
        # add transpose conv layers, with relu activation function
        x = F.relu(self.t_conv1(x))
        # output layer (with sigmoid for scaling from 0 to 1)
        x = F.sigmoid(self.t_conv2(x))

        return x

model = MT19AIE275AutoEncoder()
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(device)
model.to(device)
# Validation using BCE Loss function
criterion = torch.nn.BCELoss()

# Using an Adam Optimizer with lr = 0.1
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

num_epochs = 100

n_epochs = 100

for epoch in range(1, n_epochs+1):
    # monitor training loss
    train_loss = 0.0

    ###################
    # train the model #
    ###################
    for data in loader:
        # _ stands in for labels, here
        # no need to flatten images
        images, _ = data
        # clear the gradients of all optimized variables
        optimizer.zero_grad()
        # forward pass: compute predicted outputs by passing inputs to the model
        outputs = model(images)
        # calculate the loss
        loss = criterion(outputs, images)
        # backward pass: compute gradient of the loss with respect to model parameters
        loss.backward()
        # perform a single optimization step (parameter update)
        optimizer.step()
        # update running training loss
        train_loss += loss.item()*images.size(0)

    # print avg training statistics
    train_loss = train_loss/len(loader)
    print('Epoch: {} \tTraining Loss: {:.6f}'.format(
        epoch,
        train_loss
        ))

from ignite.engine import *
from ignite.handlers import *
from ignite.metrics import *
from ignite.utils import *
from ignite.contrib.metrics.regression import *
from ignite.contrib.metrics import *

from ignite.metrics import SSIM,PSNR
dataList = list()
targetList = list()

import math
with torch.no_grad():
    for data, _ in testLoader:
        data = data.to(device)
        recon = model(data)
        break

import matplotlib.pyplot as plt
plt.figure(dpi=250)
sum = 0
counter = 0
fig, ax = plt.subplots(2, 7, figsize=(15, 4))

for i in range(7):
    ax[0, i].imshow(data[i].cpu().numpy().transpose((1, 2, 0)))
    ax[1, i].imshow(recon[i].cpu().numpy().transpose((1, 2, 0)))

    dataList.append(data[i])
    targetList.append(recon[i])

    ax[0, i].axis('OFF')
    ax[1, i].axis('OFF')
plt.show()

datalen = len(dataList)
mse_loss = torch.nn.MSELoss()

def eval_step(engine, batch):
    return batch

default_evaluator = Engine(eval_step)

psnr = PSNR(data_range=1.0)
psnr.attach(default_evaluator, 'psnr')
SSIMetric = SSIM(data_range=1.0)

metric = MeanSquaredError()
#metric.attach(default_evaluator, 'mse')

print("Metrices")
for x in range(datalen):
  input = dataList[i]
  target = targetList[i]
  output = mse_loss(input, target)
  sum = sum + output
  counter = counter + 1
  state = default_evaluator.run([[input, target]])
  print("PSNR")
  print(state.metrics['psnr'])
 # print("SSIM")
  #print(state.metrics['ssim'])
  #print("MSE")
 # print(state.metrics['mse'])

default_evaluator2 = Engine(eval_step)
SSIMetric.attach(default_evaluator2, 'ssim')
print("SSIM")
for x in range(datalen):
  input = dataList[i]
  target = targetList[i]
  output = mse_loss(input, target)
  sum = sum + output
  counter = counter + 1
  state = default_evaluator2.run([[input, target]])
  print("SSIM")
  print(state.metrics['ssim'])





RMSE = sum/counter
RMSE_ROOT = math.sqrt(RMSE)
print("Square Mean Error")
print(RMSE)
print("RMSE: Root Mean Square Error")
print(RMSE_ROOT)

pip install pytorch-ignite

pip install Pillow

from PIL import ImageCms

class ColorTransWithFlip:

    '''Class for transforming RGB<->LAB color spaces for PIL images.'''

    def __init__(self):
      self.srgb_p = ImageCms.createProfile("sRGB")
      self.lab_p  = ImageCms.createProfile("LAB")
      self.rgb2lab_trans = ImageCms.buildTransformFromOpenProfiles(self.srgb_p, self.lab_p, "RGB", "LAB")
      self.lab2rgb_trans = ImageCms.buildTransformFromOpenProfiles(self.lab_p, self.srgb_p, "LAB", "RGB")

    def rgb2lab(self,img):

        return ImageCms.applyTransform(img, self.rgb2lab_trans)

    def lab2rgb(self,img):
        toflipImg = ImageCms.applyTransform(img, self.lab2rgb_trans)
        return transforms.functional.hflip(img)

pip install scikit-image
from skimage.io import imread
from skimage.color import rgb2lab, lab2rgb
class ColorTrans:

    '''Class for transforming RGB<->LAB color spaces for PIL images.'''

    def __init__(self):


    def lab2rgb(self,img):
      im = imread(img)
		  return lab2rgb(im)